{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mac/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import RegexpParser\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_syntax(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    grammar = r\"\"\"\n",
    "      NP: {<DT>?<JJ>*<NN.*>}\n",
    "      PP: {<IN><NP>}\n",
    "      VP: {<VB.*><NP|PP|CLAUSE>+$}\n",
    "      CLAUSE: {<NP><VP>}\n",
    "    \"\"\"\n",
    "    chunker = RegexpParser(grammar)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        tagged = pos_tag(words)\n",
    "        tree = chunker.parse(tagged)\n",
    "        \n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() in ['NP', 'VP', 'PP']:\n",
    "                current_chunk.append(\" \".join([word for word, tag in subtree.leaves()]))\n",
    "        \n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        current_chunk = []\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def chunk_by_semantic_similarity(text, threshold=0.7):\n",
    "    sentences = sent_tokenize(text)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    similarity_matrix = cosine_similarity(X)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "    \n",
    "    for i in range(1, len(sentences)):\n",
    "        sim = similarity_matrix[i-1][i]\n",
    "        if sim < threshold:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentences[i]]\n",
    "        else:\n",
    "            current_chunk.append(sentences[i])\n",
    "    \n",
    "    chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def chunk_by_topic_modeling(list_sentences, n_topics=5, chunk_size=5):\n",
    "    sentences = sent_tokenize(text)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    lda = LDA(n_components=n_topics)\n",
    "    lda.fit(X)\n",
    "    \n",
    "    topic_assignments = lda.transform(X).argmax(axis=1)\n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "    current_topic = topic_assignments[0]\n",
    "    \n",
    "    for i in range(1, len(sentences)):\n",
    "        if topic_assignments[i] != current_topic or len(current_chunk) >= chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentences[i]]\n",
    "            current_topic = topic_assignments[i]\n",
    "        else:\n",
    "            current_chunk.append(sentences[i])\n",
    "    \n",
    "    chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def optimal_contextual_chunking(text, method='semantic', **kwargs):\n",
    "    if method == 'syntax':\n",
    "        return chunk_by_syntax(text)\n",
    "    elif method == 'semantic':\n",
    "        return chunk_by_semantic_similarity(text, **kwargs)\n",
    "    elif method == 'topic':\n",
    "        return chunk_by_topic_modeling(text, **kwargs)\n",
    "    elif method == 'auto':\n",
    "        # Automatically determine best chunking strategy\n",
    "        if len(text) < 500:  # Short conversation\n",
    "            return chunk_by_syntax(text)\n",
    "        elif len(text) < 1500:  # Medium conversation\n",
    "            return chunk_by_semantic_similarity(text, threshold=kwargs.get('threshold', 0.7))\n",
    "        else:  # Long conversation\n",
    "            return chunk_by_topic_modeling(text, n_topics=kwargs.get('n_topics', 5), chunk_size=kwargs.get('chunk_size', 5))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method specified. Choose from 'syntax', 'semantic', 'topic', or 'auto'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = \"\"\"Agent: Good morning! Thank you for calling XYZ Network Services. My name is Sam. How can I assist you today?\n",
    "\n",
    "Customer: Hi, Sam. I’m really frustrated right now. I’ve been having constant issues with my internet connection for the past week, and to top it off, my bill this month is way higher than usual. I’m not sure what’s going on, but I need this fixed immediately.\n",
    "\n",
    "Agent: I’m really sorry to hear that you’ve been having such a tough time, especially with the connection and the billing. I can definitely help you out. Let’s start with the internet connection issue. Could you tell me more about the problems you’ve been experiencing?\n",
    "\n",
    "Customer: The internet keeps dropping every few minutes. Sometimes it’s just slow, but other times, it disconnects entirely. I work from home, so this is a huge problem for me. I can’t afford to keep losing connection during important meetings.\n",
    "\n",
    "Agent: I completely understand how disruptive that must be, especially when you’re relying on the internet for work. Let’s run through a few troubleshooting steps first. Have you noticed if the connection drops at specific times of the day, or is it random?\n",
    "\n",
    "Customer: It seems pretty random, honestly. But it does feel like it’s worse in the evenings, which is when I’m usually trying to finish up work.\n",
    "\n",
    "Agent: That could be due to network congestion, but we’ll check all possibilities. Let’s start by rebooting your router. Have you tried that recently?\n",
    "\n",
    "Customer: I’ve rebooted it multiple times, but it only helps for a short while. Then the same thing happens again.\n",
    "\n",
    "Agent: Thank you for trying that already. It sounds like there might be a deeper issue. I’m going to run a diagnostic on your connection from our end. This will take a minute or two. While that’s running, could you also check if all the cables are securely connected to your router and modem?\n",
    "\n",
    "Customer: Sure, give me a second… (pause) Everything seems fine with the cables.\n",
    "\n",
    "Agent: Thanks for checking. I’m seeing some irregularities on our end as well. It looks like there’s a signal issue that could be affecting your service. I’ll need to escalate this to our technical team to investigate further. They might need to send out a technician to check the lines outside your home. I can arrange that for you. What time would be convenient for a visit?\n",
    "\n",
    "Customer: If they could come tomorrow morning, that would be great. I’m just so tired of dealing with this. It’s been affecting my work, and I’ve even had to use my mobile data as a backup, which is why my bill is so high this month!\n",
    "\n",
    "Agent: I can imagine how frustrating that must be. I’ll schedule the technician for tomorrow morning between 9 and 11 AM. Regarding your bill, let’s take a closer look at that next. You mentioned it was higher than usual—did you see any unexpected charges on it?\n",
    "\n",
    "Customer: Yeah, I noticed I was charged extra for data usage, which is ridiculous since I’m already paying for unlimited internet.\n",
    "\n",
    "Agent: I see why that would be concerning. Let me pull up your billing details… (pause) It looks like the extra charges are indeed for mobile data usage. While your home internet plan is unlimited, mobile data incurs additional charges if you exceed your plan’s limit. However, since this was due to the service issues, I can offer you a credit for the extra charges. Does that sound acceptable?\n",
    "\n",
    "Customer: Yes, that’s fair. But I still feel like I shouldn’t have to deal with these issues in the first place. And this isn’t the first time I’ve had problems with your service. I’m seriously considering switching providers.\n",
    "\n",
    "Agent: I completely understand your frustration, and I’m sorry that our service hasn’t met your expectations. Your experience is very important to us, and we’d hate to lose you as a customer. If it’s alright with you, I can check if there are any upgrades or promotions available that might improve your service and save you some money.\n",
    "\n",
    "Customer: Well, I’m open to hearing about what you have to offer, but honestly, I just need reliable internet.\n",
    "\n",
    "Agent: Reliability is key, and we want to make sure you’re getting the best possible service. Let me check our current offers… (pause) We have an upgraded plan that includes faster internet speeds and a more advanced router, which could help with the connection stability. There’s also a discount available if you bundle this with your mobile plan. Would you like to hear more details?\n",
    "\n",
    "Customer: Faster speeds sound good, but I’m concerned about the cost. I don’t want to end up paying more than I already am.\n",
    "\n",
    "Agent: That’s a valid concern. The upgraded plan would be an additional $10 per month, but with the bundle discount, it would actually be $5 less than what you’re paying right now for both services. Plus, with the new router, we might be able to eliminate the connection issues you’ve been facing.\n",
    "\n",
    "Customer: Okay, that sounds reasonable. But will this really fix the problem? I don’t want to end up in the same situation after upgrading.\n",
    "\n",
    "Agent: I understand your hesitation. The upgraded router has better range and stability, which should make a noticeable difference. Also, with the technician coming out tomorrow, we’ll be able to address any underlying issues that might be affecting your current setup. If the problems persist even after the upgrade, we have a 30-day satisfaction guarantee, so you can switch back to your previous plan or explore other options without any penalty.\n",
    "\n",
    "Customer: Alright, let’s go ahead with the upgrade then. I just hope this finally resolves everything.\n",
    "\n",
    "Agent: I’m confident this will improve your experience, but I’ll also follow up with you personally after the technician’s visit to ensure everything is working as it should. I’ll process the upgrade now… (pause) The upgrade is complete, and you should see the new equipment delivered within the next two days. Is there anything else I can assist you with today?\n",
    "\n",
    "Customer: Actually, yes. I’ve been with XYZ Network Services for a long time, but I never really looked into my contract details. Can you tell me if I’m locked into any specific term or if I’m free to cancel if things don’t improve?\n",
    "\n",
    "Agent: Let me pull up your contract details… (pause) You’re currently on a month-to-month plan, which means you’re not locked into a long-term contract. You’re free to cancel at any time with no penalty. However, I hope the changes we’re making will improve your service so you don’t feel the need to switch.\n",
    "\n",
    "Customer: That’s good to know. I really hope this works out because, to be honest, I don’t have the time or energy to go through the hassle of switching providers.\n",
    "\n",
    "Agent: I completely understand, and we’ll do everything we can to ensure you don’t have to go through that. Just to recap, we’ve scheduled a technician for tomorrow morning, processed the upgrade to your plan, and applied a credit for the extra charges on your bill. Is there anything else on your mind that I can help with?\n",
    "\n",
    "Customer: I think that covers everything for now. I’ll wait to see how the technician’s visit goes and how the new setup works out.\n",
    "\n",
    "Agent: Great! I’ll follow up with you after the technician’s visit to make sure everything is resolved. If you have any more questions or concerns, don’t hesitate to reach out. Thank you for giving us the chance to make things right, and I hope you have a better experience moving forward.\n",
    "\n",
    "Customer: Thanks, Sam. I appreciate your help. Have a good day.\n",
    "\n",
    "Agent: You too! Take care.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mac/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/mac/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /Users/mac/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_semantic_similarity(sentences, threshold=0.7):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    similarity_matrix = cosine_similarity(X)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "    \n",
    "    for i in range(1, len(sentences)):\n",
    "        sim = similarity_matrix[i-1][i]\n",
    "        if sim < threshold:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentences[i]]\n",
    "        else:\n",
    "            current_chunk.append(sentences[i])\n",
    "    \n",
    "    chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def chunk_by_noun_verb_ner(chunks):\n",
    "    refined_chunks = []\n",
    "    \n",
    "    def extract_entities(chunk):\n",
    "        words = word_tokenize(chunk)\n",
    "        tagged = pos_tag(words)\n",
    "        entities = [word for word, pos in tagged if pos.startswith('NN') or pos.startswith('VB')]\n",
    "        return entities\n",
    "    \n",
    "    # Process each chunk and compare with the next chunk based on shared entities\n",
    "    if chunks:\n",
    "        current_chunk = chunks[0]\n",
    "        current_entities = set(extract_entities(current_chunk))\n",
    "    \n",
    "        for i in range(1, len(chunks)):\n",
    "            next_chunk = chunks[i]\n",
    "            next_entities = set(extract_entities(next_chunk))\n",
    "            \n",
    "            # If there are shared entities, merge the chunks\n",
    "#             print(current_entities.intersection(next_entities))\n",
    "            if len(current_entities.intersection(next_entities))>6:\n",
    "                current_chunk += \" \" + next_chunk\n",
    "                current_entities.update(next_entities)\n",
    "            else:\n",
    "                refined_chunks.append(current_chunk)\n",
    "                current_chunk = next_chunk\n",
    "                current_entities = next_entities\n",
    "        \n",
    "        # Append the last chunk\n",
    "        refined_chunks.append(current_chunk)\n",
    "    \n",
    "    return refined_chunks\n",
    "\n",
    "\n",
    "def chunk_by_topic_modeling(chunks, n_topics=10, chunk_size=5):\n",
    "    refined_chunks = []\n",
    "    n_topics= int(len(chunks)*0.3)\n",
    "    # Vectorize the chunks\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(chunks)\n",
    "    \n",
    "    # Apply LDA for topic modeling\n",
    "    lda = LDA(n_components=n_topics)\n",
    "    lda.fit(X)\n",
    "    \n",
    "    # Get topic assignments for each chunk\n",
    "    topic_assignments = lda.transform(X).argmax(axis=1)\n",
    "    \n",
    "    current_chunk = [chunks[0]]\n",
    "    current_topic = topic_assignments[0]\n",
    "    \n",
    "    for i in range(1, len(chunks)):\n",
    "        if topic_assignments[i] != current_topic or len(current_chunk) >= chunk_size:\n",
    "            refined_chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [chunks[i]]\n",
    "            current_topic = topic_assignments[i]\n",
    "        else:\n",
    "            current_chunk.append(chunks[i])\n",
    "    \n",
    "    # Append the last chunk\n",
    "    refined_chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return refined_chunks\n",
    "\n",
    "def chunk_by_length(refined_chunks, min_length=100, max_length=600):\n",
    "    final_chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for chunk in refined_chunks:\n",
    "        if len(chunk) > max_length:\n",
    "            if current_chunk:\n",
    "                final_chunks.append(current_chunk)\n",
    "                current_chunk = \"\"\n",
    "\n",
    "            # Split large chunks directly into smaller pieces\n",
    "            while len(chunk) > max_length:\n",
    "                part = chunk[:max_length]\n",
    "                final_chunks.append(part)\n",
    "                chunk = chunk[max_length:]\n",
    "\n",
    "            if len(chunk) > 0:\n",
    "                current_chunk = chunk\n",
    "        elif len(chunk) + len(current_chunk) <= max_length:\n",
    "            # Add to the current chunk if it won't exceed max_length\n",
    "            current_chunk += \" \" + chunk if current_chunk else chunk\n",
    "        else:\n",
    "            # If the current chunk is too small, append it to the previous chunk\n",
    "            if len(current_chunk) < min_length and final_chunks:\n",
    "                final_chunks[-1] += \" \" + current_chunk\n",
    "            else:\n",
    "                final_chunks.append(current_chunk)\n",
    "            current_chunk = chunk\n",
    "\n",
    "    # Handle any remaining chunk\n",
    "    if current_chunk:\n",
    "        if len(current_chunk) < min_length and final_chunks:\n",
    "            final_chunks[-1] += \" \" + current_chunk\n",
    "        else:\n",
    "            final_chunks.append(current_chunk)\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "\n",
    "def optimal_sequential_contextual_chunking(text):\n",
    "    sentences = [x for x in conversation.split('\\n') if len(x)>3]\n",
    "    refined_chunks = chunk_by_noun_verb_ner(sentences)\n",
    "    refined_chunks = chunk_by_topic_modeling(refined_chunks)\n",
    "    refined_chunks = chunk_by_semantic_similarity(refined_chunks, threshold=0.6)\n",
    "    final_chunks = chunk_by_length(refined_chunks)\n",
    "    \n",
    "    for i, chunk in enumerate(final_chunks):\n",
    "        print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n",
    "    \n",
    "    return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/mac/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m conversation\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m refined_chunks \u001b[38;5;241m=\u001b[39m chunk_by_noun_verb_ner(sentences)\n\u001b[1;32m      3\u001b[0m refined_chunks \u001b[38;5;241m=\u001b[39m chunk_by_semantic_similarity(refined_chunks, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m      4\u001b[0m refined_chunks \u001b[38;5;241m=\u001b[39m chunk_by_topic_modeling(refined_chunks)\n",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m, in \u001b[0;36mchunk_by_noun_verb_ner\u001b[0;34m(chunks)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunks:\n\u001b[1;32m     32\u001b[0m     current_chunk \u001b[38;5;241m=\u001b[39m chunks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 33\u001b[0m     current_entities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(extract_entities(current_chunk))\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(chunks)):\n\u001b[1;32m     36\u001b[0m         next_chunk \u001b[38;5;241m=\u001b[39m chunks[i]\n",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m, in \u001b[0;36mchunk_by_noun_verb_ner.<locals>.extract_entities\u001b[0;34m(chunk)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_entities\u001b[39m(chunk):\n\u001b[0;32m---> 25\u001b[0m     words \u001b[38;5;241m=\u001b[39m word_tokenize(chunk)\n\u001b[1;32m     26\u001b[0m     tagged \u001b[38;5;241m=\u001b[39m pos_tag(words)\n\u001b[1;32m     27\u001b[0m     entities \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word, pos \u001b[38;5;129;01min\u001b[39;00m tagged \u001b[38;5;28;01mif\u001b[39;00m pos\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m pos\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVB\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/mac/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "sentences = [x for x in conversation.split('\\n') if len(x)>3]\n",
    "refined_chunks = chunk_by_noun_verb_ner(sentences)\n",
    "refined_chunks = chunk_by_semantic_similarity(refined_chunks, threshold=0.8)\n",
    "refined_chunks = chunk_by_topic_modeling(refined_chunks)\n",
    "final_chunks = chunk_by_length(refined_chunks)\n",
    "for i, chunk in enumerate(final_chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
