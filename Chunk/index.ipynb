{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk and Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(pdf_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_prompt(forward_summary, current_chunk):\n",
    "    template = \"\"\"\n",
    "    Given the following forward summary and a new text chunk, generate a concise summary of the chunk that maintains coherence with the forward summary.\n",
    "\n",
    "    Forward Summary: {forward_summary}\n",
    "    Current Chunk: {current_chunk}\n",
    "\n",
    "    Please provide a clear and relevant summary of the current chunk.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(input_variables=[\"forward_summary\", \"current_chunk\"], template=template)\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunk(forward_summary, chunk, llm):\n",
    "    prompt = create_summary_prompt(forward_summary, chunk)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    summary = chain.run({\"forward_summary\": forward_summary, \"current_chunk\": chunk})\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document into manageable chunks\n",
    "def split_text(document, chunk_size=1000):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(document)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main function to process the document and generate summaries\n",
    "def process_pdf_document(pdf_path):\n",
    "    # Load PDF\n",
    "    documents = load_pdf(pdf_path)\n",
    "    \n",
    "    # Initialize OpenAI model\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    # Split the document into chunks\n",
    "    chunks = split_text(documents, chunk_size=1000)\n",
    "    forward_summary = \"\"\n",
    "    chunk_summaries = []\n",
    "\n",
    "    # Process each chunk iteratively\n",
    "    for chunk in chunks[0:10]:\n",
    "        current_text = chunk.page_content\n",
    "        chunk_summary = summarize_chunk(forward_summary, current_text, llm)\n",
    "        \n",
    "        # Update forward summary\n",
    "        forward_summary = chunk_summary\n",
    "        \n",
    "        # Store the summary for this chunk\n",
    "        chunk_summaries.append(chunk_summary)\n",
    "\n",
    "    return chunk_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./docs/GIÁO-TRÌNH-PHÂN-TÍCH-DỮ-LIỆU-KINH-DOANH.pdf\"\n",
    "summaries = process_pdf_document(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Chunk 1: The chunk shows a title page for a Business Data Analysis textbook, listing authors (Nguyễn Đình Thuân, Nguyễn Minh Nhựt, Nguyễn Thị Viết Hương, Trịnh Thị Thanh Trúc) and the publishing institution (University of Information Technology, Ho Chi Minh City University of Technology).  The year is 2024.\n",
      "\n",
      "Summary of Chunk 2: The preface explains the increasing importance of data analysis in the context of Industry 4.0.  This Business Data Analysis textbook, written from an IT perspective, aims to equip students with the necessary statistical, algorithmic, and programming skills using tools like R and Python.  The textbook covers descriptive statistics and data interpretation, starting with an overview of data analysis problems.\n",
      "\n",
      "Summary of Chunk 3: The textbook covers data analysis using statistics and machine learning.  Chapters include descriptive statistics, data interpretation, regression analysis, logistic regression, time series forecasting, and machine learning for prediction.  Each chapter includes QR codes linking to datasets and R/Python code examples for solving data analysis problems.\n",
      "\n",
      "Summary of Chunk 4: The authors express hope that the textbook will help readers become excellent data analysts contributing to sustainable business development and welcome reader feedback for future improvements.\n",
      "\n",
      "Summary of Chunk 5: The chunk presents a table of contents (MỤC LỤC) for Chapter 1, covering an overview of data analysis problems and descriptive statistics.  Specific topics include an overview of data analysis, data and data transformations, and basic analytical methods including cluster and group analysis.\n",
      "\n",
      "Summary of Chunk 6: This section of the table of contents lists advanced analytical methods covered in Chapter 1, including group analysis, regression analysis, artificial neural networks, factor analysis, data mining, text analysis, time series analysis, and decision trees.\n",
      "\n",
      "Summary of Chunk 7: This chunk details sections on decision trees, attribute analysis, data measurement (including measures of central tendency, variability, and distribution shape), and data visualization (specifically bar charts).\n",
      "\n",
      "Summary of Chunk 8: The chunk introduces box plots (1.5.2),  introductions to R and Python (1.6), and practical exercises using both languages for descriptive statistics and visualization (1.7).  It concludes with chapter exercises (1.8) and begins Chapter 2 on explanatory data analysis (2.1) focusing on estimating population values.\n",
      "\n",
      "Summary of Chunk 9: This chunk details Chapter 2's introduction to estimating population values (2.1), specifically covering estimating population means and proportions, illustrated with an example.  It then moves on to parametric tests within a population (2.2), including tests for population means (one and two sample), and variance.\n",
      "\n",
      "Summary of Chunk 10: This chunk outlines the remaining sections of Chapter 2, covering variance tests (2.2.3), analysis of variance (ANOVA) including one-way ANOVA (2.3), post-hoc Tukey test (2.3.3), two-way ANOVA (2.3.4), analysis of covariance (2.3.5), and finally, Chi-Square tests (2.4).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, summary in enumerate(summaries):\n",
    "    print(f\"Summary of Chunk {i+1}: {summary}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Artificial Intelligence and Machine Learning are transforming the tech industry.\",\n",
    "    \"The use of machine learning in healthcare has great potential.\",\n",
    "    \"Robotics and AI have been integrated into various industries.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract domain-specific tags (keywords, entities, etc.)\n",
    "def extract_tags_from_text(text):\n",
    "    doc = nlp(text)\n",
    "    # Extract named entities and noun phrases (common domain-specific terms)\n",
    "    tags = [ent.text.lower() for ent in doc.ents] + [np.text.lower() for np in doc.noun_chunks]\n",
    "    return tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tags = []\n",
    "for doc in corpus:\n",
    "    tags = extract_tags_from_text(doc)\n",
    "    corpus_tags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_tags = [tag for tags in corpus_tags for tag in tags]\n",
    "tag_frequency = Counter(flat_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common tags in corpus: [('machine learning', 2), ('ai', 2), ('artificial intelligence and machine learning', 1), ('artificial intelligence', 1), ('the tech industry', 1), ('the use', 1), ('healthcare', 1), ('great potential', 1), ('robotics', 1), ('various industries', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Most common tags in corpus:\", tag_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tags_from_query(query):\n",
    "    return extract_tags_from_text(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can machine learning help in improving healthcare?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags from query: ['help', 'healthcare']\n"
     ]
    }
   ],
   "source": [
    "query_tags = extract_tags_from_query(query)\n",
    "print(\"Tags from query:\", query_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_query_to_corpus_tags(query_tags, corpus_tags):\n",
    "    matched_tags = []\n",
    "    for tags in corpus_tags:\n",
    "        matched_tags.append([tag for tag in tags if tag in query_tags])\n",
    "    return matched_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped tags from query to corpus: [[], ['healthcare'], []]\n"
     ]
    }
   ],
   "source": [
    "mapped_tags = map_query_to_corpus_tags(query_tags, corpus_tags)\n",
    "print(\"Mapped tags from query to corpus:\", mapped_tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
